ğŸ”¥ 100 Adjetivos essenciais da Ã¡rea de TI

---
ğŸ”µ Com KEEP (manter, continuar, persistir)
keep up â€“ manter o ritmo / continuar
â€œWe need to keep up with the new compliance requirements.â€
â€œGood job keeping up with the alerts yesterday.â€
keep track of â€“ acompanhar / rastrear
â€œCan you keep track of the Kafka offsets in the consumer group?â€
â€œIâ€™m keeping track of all retries in the logs.â€
keep an eye on â€“ ficar de olho
â€œLetâ€™s keep an eye on the CPU usage after the deploy.â€
keep going â€“ continuar trabalhando / seguir
â€œThe pipeline failed, but we can keep going locally.â€
keep something in mind â€“ ter em mente
â€œKeep in mind that this endpoint is synchronous.â€
keep at it â€“ persistir
â€œKeep at it, the root cause will show up in the logs.â€

ğŸŸ£ Com SET (configurar, definir)
set up â€“ configurar / preparar / montar
â€œIâ€™ll set up the new SQS queues.â€
set out â€“ definir objetivos
â€œWe set out to reduce latency in this sprint.â€
set off â€“ disparar / iniciar
â€œThis change can set off multiple retries in the service.â€
set aside â€“ reservar tempo
â€œLetâ€™s set aside 30 minutes to review the architecture.â€

ğŸŸ¢ Com GET (obter, receber, entender)
get back to â€“ retornar / responder depois
â€œIâ€™ll get back to you after checking the logs.â€
get rid of â€“ remover / eliminar
â€œWe need to get rid of this deprecated endpoint.â€
get into â€“ entrar no assunto / se aprofundar
â€œLetâ€™s not get into performance tuning yet.â€
get around â€“ contornar problema
â€œWe can get around this by caching the response.â€
get through â€“ finalizar / conseguir passar
â€œThe request canâ€™t get through the gateway.â€
get ahead of â€“ antecipar problemas
â€œWe need to get ahead of this incident before peak time.â€
get stuck â€“ travar / ficar bloqueado
â€œI got stuck debugging the token validation.â€

ğŸ”¥ Outros phrasal verbs extremamente comuns em TI
ğŸŸ¨ AÃ§Ã£o tÃ©cnica
look into â€“ investigar
â€œWeâ€™ll look into the timeout on the login endpoint.â€
figure out â€“ descobrir / entender
â€œLetâ€™s figure out why WebClient isnâ€™t retrying.â€
track down â€“ rastrear
â€œWe tracked down the root cause to a misconfigured Redis key.â€
bring up (serviÃ§o) â€“ subir
â€œThe pod didnâ€™t bring up correctly after the deploy.â€
bring down â€“ derrubar / desligar serviÃ§o
â€œWe need to bring down the instance before patching.â€
roll out â€“ liberar versÃ£o
â€œThe new flow will be rolled out gradually.â€
roll back â€“ reverter
â€œWe rolled back due to high error rate.â€
clean up â€“ limpar / remover lixo
â€œWe should clean up unused feature flags.â€

ğŸŸ© ComunicaÃ§Ã£o entre squads
follow up â€“ acompanhar / cobrar
â€œIâ€™ll follow up with the BFF team about the contract.â€
point out â€“ destacar
â€œJust pointing out: this API is not idempotent.â€
check in â€“ sincronizar / atualizar status
â€œLetâ€™s check in after lunch to finalize the review.â€
hand over â€“ repassar
â€œIâ€™ll hand over the logs to SRE.â€
talk through â€“ explicar passo a passo
â€œLet me talk you through the authentication flow.â€
 
---
phrasal verbs

1. set up â€“ configurar / montar
   to set up a server, to set up a project
   We need to set up a new environment for QA.
   (A gente precisa configurar um novo ambiente para QA.)
   Can you set up the Spring Boot project with Docker from the beginning?
   (VocÃª pode montar o projeto Spring Boot com Docker desde o inÃ­cio?)

2. spin up â€“ subir (rÃ¡pido) um recurso/serviÃ§o
   Muito usado pra containers, VMs, pods, etc.
   We can spin up a new pod in the cluster to handle the extra load.
   (Podemos subir um novo pod no cluster pra aguentar a carga extra.)
   They spin up a test database before running the integration tests.
   (Eles sobem um banco de teste antes de rodar os testes de integraÃ§Ã£o.)

3. scale up / scale down â€“ escalar pra cima / pra baixo
   If traffic increases, weâ€™ll scale up the number of instances.
   (Se o trÃ¡fego aumentar, vamos escalar o nÃºmero de instÃ¢ncias.)
   At night we usually scale down the services to save costs.
   (Ã€ noite geralmente reduzimos a escala dos serviÃ§os pra economizar.)

4. roll out â€“ colocar em produÃ§Ã£o / liberar
   Weâ€™re going to roll out the new version tonight.
   (Vamos liberar a nova versÃ£o hoje Ã  noite.)
   The feature was rolled out to 10% of the users first.
   (A feature foi liberada primeiro para 10% dos usuÃ¡rios.)

5. roll back â€“ reverter deploy / versÃ£o
   We had to roll back the deployment because of a critical bug.
   (Tivemos que reverter o deploy por causa de um bug crÃ­tico.)
   If the health checks fail, the pipeline automatically rolls back.
   (Se os health checks falham, o pipeline reverte automaticamente.)

6. bring up / bring down â€“ subir / derrubar serviÃ§o
   The service is down, weâ€™re trying to bring it back up.
   (O serviÃ§o estÃ¡ fora, estamos tentando subir de novo.)
   We need to bring down the instance to apply the patch.
   (Precisamos derrubar a instÃ¢ncia pra aplicar o patch.)

7. shut down â€“ desligar / encerrar
   Weâ€™ll shut down the old legacy service next quarter.
   (Vamos desligar o serviÃ§o legado no prÃ³ximo trimestre.)
   The app shuts down if it canâ€™t connect to the database.
   (O app encerra se nÃ£o conseguir conectar no banco.)

8. log in / log out â€“ entrar / sair (autenticaÃ§Ã£o)
   Users canâ€™t log in after the last deployment.
   (Os usuÃ¡rios nÃ£o conseguem fazer login depois do Ãºltimo deploy.)
   The token expires and the user is logged out automatically.
   (O token expira e o usuÃ¡rio Ã© deslogado automaticamente.)

9. sign up â€“ cadastrar-se / criar conta
   We added a new flow for users to sign up with Google.
   (Adicionamos um novo fluxo pra usuÃ¡rios se cadastrarem com o Google.)
   The conversion rate on the sign-up page improved after the redesign.
   (A taxa de conversÃ£o na pÃ¡gina de cadastro melhorou depois do redesign.)

10. back up â€“ fazer backup
    We back up the database every night.
    (Fazemos backup do banco toda noite.)
    Before changing the schema, please back up the data.
    (Antes de mudar o schema, faz um backup dos dados.)

11. figure out â€“ entender / descobrir / destrinchar
    Muito usado pra debug e anÃ¡lise.
    We need to figure out why the API is timing out.
    (Precisamos entender por que a API estÃ¡ dando timeout.)
    Iâ€™m still trying to figure out whatâ€™s causing this memory leak.
    (Ainda estou tentando descobrir o que estÃ¡ causando esse memory leak.)

12. find out â€“ descobrir (obter informaÃ§Ã£o)
    Letâ€™s find out which service is causing the high CPU usage.
    (Vamos descobrir qual serviÃ§o estÃ¡ causando o alto uso de CPU.)
    Iâ€™ll find out who changed this configuration in Kubernetes.
    (Vou descobrir quem mudou essa configuraÃ§Ã£o no Kubernetes.)

13. track down â€“ rastrear / localizar a origem
    We need to track down where this null value comes from.
    (Precisamos rastrear de onde vem esse valor null.)
    The logs helped us track down the failing dependency.
    (Os logs ajudaram a localizar a dependÃªncia que estava falhando.)

14. clean up â€“ limpar / organizar cÃ³digo, dados, logs
    We should clean up unused feature flags in the config.
    (DevÃ­amos limpar os feature flags nÃ£o usados na config.)
    Iâ€™ll clean up the controller and move this logic to a service.
    (Vou dar uma limpada no controller e mover essa lÃ³gica pra um service.)

15. break down â€“ decompor / explicar parte por parte
    Let me break down the flow: first the request hits the gateway, then the BFF, then the core service.
    (Deixa eu decompor o fluxo: primeiro a requisiÃ§Ã£o passa no gateway, depois no BFF, depois no serviÃ§o core.)
    We broke down the monolith into three microservices.
    (NÃ³s quebramos o monÃ³lito em trÃªs microsserviÃ§os.)

16. hand over â€“ repassar / passar adiante
    Iâ€™ll hand over this task to the DevOps team.
    (Vou repassar essa tarefa pro time de DevOps.)
    Can you hand over the documentation to the new developer?
    (VocÃª pode passar a documentaÃ§Ã£o pro novo desenvolvedor?)

17. follow up â€“ acompanhar / cobrar depois
    Iâ€™ll follow up on this incident after we get more logs.
    (Vou acompanhar esse incidente depois que tivermos mais logs.)
    Can you follow up with the other squad about the API contract?
    (VocÃª pode dar um follow up com a outra squad sobre o contrato da API?)

18. work around â€“ contornar (um problema)
    We added a feature flag to work around the bug in the legacy service.
    (Adicionamos um feature flag pra contornar o bug no serviÃ§o legado.)
    For now, weâ€™ll work around the limitation of this API.
    (Por enquanto, vamos contornar a limitaÃ§Ã£o dessa API.)
---

â­ 1â€“10: Estruturais
1. even
   Squad: The issue happens even after the hotfix.
   Java: The endpoint is slow even with caching enabled.

2. still
   Squad: The bug is still open on Jira.
   Java: The service still returns 500.

3. yet
   Squad: The PO hasnâ€™t confirmed the requirement yet.
   Java: The consumer hasnâ€™t started processing messages yet.

4. though
   Squad: It looks stable now, though the logs are strange.
   Java: The query works, though itâ€™s not optimized.

5. although
   Squad: Although itâ€™s late, we need this alignment.
   Java: Although the API is valid, the mapper fails.

6. actually
   Squad: Actually, we donâ€™t need another meeting.
   Java: Actually, the bug was in the DTO, not the controller.

7. basically
   Squad: Basically, we need better communication.
   Java: Basically, the service fetches, maps, and returns.

8. literally
   Squad: We literally had three incidents today.
   Java: The JVM literally hit 100% CPU.

9. already
   Squad: I already updated the ticket.
   Java: The service already consumed the message.

10. instead
    Squad: Letâ€™s try this approach instead.
    Java: Use WebClient instead of RestTemplate.

â­ 11â€“20: ComunicaÃ§Ã£o & alinhamento
11. anyway
    Squad: Anyway, letâ€™s move to the next topic.
    Java: Anyway, the retry logic still needs improvement.

12. exactly
    Squad: Exactly! Thatâ€™s the point I was making.
    Java: Exactly where the timeout happens is in this method.

13. probably
    Squad: Heâ€™s probably in another call.
    Java: The failure is probably in the database call.

14. honestly
    Squad: Honestly, we don't need this extra ceremony.
    Java: Honestly, this code needs a full refactor.

15. obviously
    Squad: Obviously, we need more test coverage.
    Java: Obviously, JPA canâ€™t handle this structure.

16. apparently
    Squad: Apparently, the deploy didnâ€™t run.
    Java: Apparently, the header isnâ€™t being sent.

17. definitely
    Squad: We definitely need to fix this today.
    Java: This logic definitely needs validation.

18. eventually
    Squad: Eventually, weâ€™ll migrate to a new platform.
    Java: Eventually, this service will be rewritten.

19. currently
    Squad: Currently, we have three open incidents.
    Java: The service is currently failing on startup.

20. recently
    Squad: Recently, weâ€™ve had many access issues.
    Java: The logs recently started showing this warning.

â­ 21â€“30: ReuniÃµes / fluxo
21. however
    Squad: We delivered the feature; however, QA found issues.
    Java: The call works; however, the mapping is wrong.

22. therefore
    Squad: Therefore, we need more people on this task.
    Java: The service failed; therefore, the fallback triggered.

23. meanwhile
    Squad: Meanwhile, the mobile team is testing.
    Java: Meanwhile, the scheduler keeps sending events.

24. otherwise
    Squad: We must sync this today, otherwise itâ€™ll block the sprint.
    Java: Add validation, otherwise the API will break.

25. moreover
    Squad: Moreover, this change affects other squads.
    Java: Moreover, this method is used by three services.

26. besides
    Squad: Besides that, thereâ€™s nothing new.
    Java: Besides this error, everything else works.

27. regarding
    Squad: Regarding the incident, we need a timeline.
    Java: Regarding this class, letâ€™s add some logs.

28. related
    Squad: Itâ€™s related to the last deploy.
    Java: The issue is related to the thread pool.

29. considering
    Squad: Considering the deadline, we need focus.
    Java: Considering the payload size, we need compression.

30. depending
    Squad: Depending on the PO, we can move forward.
    Java: Depending on the header, the API returns another format.

â­ 31â€“40: AÃ§Ãµes tÃ©cnicas
31. handle
    Squad: We need to handle this in todayâ€™s refinement.
    Java: The service must handle null values.

32. fetch
    Squad: We fetch data from two providers.
    Java: The repository fetches all active records.

33. trigger
    Squad: This deploy will trigger a regression test.
    Java: The scheduler triggers the batch job.

34. expose
    Squad: We expose only public endpoints.
    Java: This controller exposes two GET APIs.

35. consume
    Squad: The BI team will consume the data.
    Java: The listener consumes messages from Kafka.

36. provide
    Squad: This dashboard provides visibility.
    Java: The adapter provides the implementation for the port.

37. ensure
    Squad: We need to ensure alignment with mobile.
    Java: Ensure the service returns a valid DTO.

38. avoid
    Squad: Letâ€™s avoid last-minute changes.
    Java: Avoid calling this API synchronously.

39. improve
    Squad: We must improve our communication.
    Java: Letâ€™s improve the error handling.

40. increase
    Squad: Can we increase the priority of this task?
    Java: Increase the timeout for this external API.

â­ 41â€“50: Problemas e incidentes
41. failure
    Squad: We had a failure in production.
    Java: The failure occurred during the database call.

42. issue
    Squad: This issue affects two squads.
    Java: The issue is in the mapping layer.

43. outage
    Squad: The outage impacted several teams.
    Java: The service crashed during the outage.

44. fallback
    Squad: The fallback didnâ€™t trigger correctly.
    Java: The fallback returns cached data.

45. retry
    Squad: We need a retry strategy.
    Java: The client retries three times before failing.

46. timeout
    Squad: Weâ€™re seeing timeout spikes.
    Java: The request hits a timeout on the provider.

47. request
    Squad: The request volume increased today.
    Java: The request contains an invalid header.

48. response
    Squad: The response time is too high.
    Java: The response body is missing fields.

49. payload
    Squad: The payload changed without notice.
    Java: The payload doesnâ€™t match the DTO.

50. environment
    Squad: Which environment are you testing in?
    Java: This config only works in the QA environment.

---

Are you out partying?
"Are you out partying? We need you on the call â€” the Kafka consumer is stuck again."
"Are you out partying? Because you sound way too happy for someone debugging a legacy SOAP service."

brand new

Sort of
"Did we finish the deployment? â€” Sort of. The app is up, but the health check is still failing."
"Is this what the PO wanted? â€” Sort of. Weâ€™ll need more clarification in refinement."

Spooky
"The logs look spooky â€” thereâ€™s no stack trace, but the request still fails."
"Itâ€™s kind of spooky how the issue disappears when we turn on debug mode."

Guess what?
"Guess what? The test passed locally but exploded in QA again."
"Guess what? The RabbitMQ message wasnâ€™t the problem â€” it was the mapper layer."

You seem calmer
"You seem calmer today â€” did you finally get the Redis cache working?"
"You seem calmer now that we refactored the service. The old version was stressful for everyone."

This isnâ€™t a lecture
"Relax, this isnâ€™t a lecture â€” I just want to understand why weâ€™re using this approach."
"Look, this isnâ€™t a lecture, but we need to document this flow before someone breaks it again."

My peer
"My peer on the other squad already implemented the producer; we just need to handle the consumer."
"I talked to my peer from the mobile team â€” theyâ€™re waiting for our API contract."
â€œI discussed this approach with my peer, and we agreed to refactor the service.â€
â€œMy peer and I are reviewing the new architecture proposal.â€
â€œOne of my peers found a bug in the Kafka consumer.â€
â€œI paired with a peer to debug the integration issue.â€

Rather than
"Letâ€™s use a DTO rather than exposing the entity directly."
"We should use async processing rather than blocking the thread during the external call."

regret
â€œI regret not implementing proper logging earlier; it would have saved us hours during the incident.â€
â€œWe regret deploying without a full rollback plan â€” it taught the team an important lesson about release safety.â€

Predict
"We canâ€™t really predict how the service will behave under load until we run the stress tests."
"Itâ€™s hard to predict the impact of this change, so let's monitor it closely after the deploy."
"We need better metrics to predict when the thread pool starts getting saturated."
"You can predict a failure here â€” this synchronous call inside the loop will slow everything down."

Pretend
"Letâ€™s pretend the external API is down and simulate the fallback logic in QA."
"In the unit test, we pretend the database returns no records to validate the edge case."
"Donâ€™t pretend everything is fine â€” the latency spike is real and getting worse."
"We canâ€™t pretend this is a small fix; this refactor will impact three other services."

Intend = pretender (no sentido de ter intenÃ§Ã£o)
I intend to finish this feature today.
â†’ Eu pretendo finalizar essa feature hoje.
We donâ€™t intend to rewrite this service now.
â†’ NÃ³s nÃ£o pretendemos reescrever esse serviÃ§o agora.

noticed
As a verb (perceber / notar)
â€œI noticed the service started responding slower after the last deploy.â€
â€œLet me know if you notice any unusual logs in the consumer.â€
â€œWe noticed a spike in latency in the authentication endpoint.â€
â€œDid you notice that the cache wasnâ€™t refreshed properly?â€
As a verb (observar / reparar em algo)
â€œI noticed you removed the null-check â€” was that intentional?â€
â€œShe noticed the feature flag wasnâ€™t being applied in production.â€
As a noun (aviso / notificaÃ§Ã£o)
â€œWe received a notice from the security team about outdated dependencies.â€
â€œThe system sent a notice about the failed batch job.â€
â€œYouâ€™ll get a notice when the deployment finishes.â€
ğŸ  Everyday examples
â€œDid you notice how quiet the neighborhood is today?â€
â€œI didnâ€™t notice the time passing.â€
â€œHe didnâ€™t even notice that it was raining.â€
â€œI noticed your new haircut â€” looks good!â€
NOTICE = perceber algo com os sentidos
REALIZE = perceber algo intelectualmente / entender

Accurate â€“ preciso Ã¡quiurÃ©t
â€¢ Our reconciliation service needs accurate timestamps to avoid processing the same Kafka message twice.
â€¢ The fraud engine only reacts when the customer provides accurate personal data.

Active â€“ ativo
â€¢ The health-check endpoint returns the number of active threads in the worker pool.
â€¢ Only active sessions are kept in Redis; expired ones are removed automatically.

Advanced â€“ avanÃ§ado
â€¢ We enabled advanced retry logic using Spring Retry to handle intermittent timeouts.
â€¢ The team is adopting advanced monitoring with OTel to trace distributed flows.

Agile â€“ Ã¡gil
â€¢ An agile backend needs small, independent modules that deploy without friction.
â€¢ We follow an agile approach to refactor features incrementally in production.

Automated â€“ automatizado
â€¢ The deployment pipeline is fully automated using Jenkins and GitHub Actions.
â€¢ Automated tests run on every PR to prevent regressions in the core domain.

Available â€“ disponÃ­vel
â€¢ The cache must remain available even when the database is under heavy load.
â€¢ We replicate the service in two zones to stay available during upgrades.

Basic â€“ bÃ¡sico
â€¢ The basic flow only validates the payload before sending it to Kafka.
â€¢ Our basic authentication layer will be replaced with OAuth2.

Blocked â€“ bloqueado
â€¢ The thread pool got blocked due to a slow external API.
â€¢ Requests were blocked while waiting for the lock on the shared resource.

Broken â€“ quebrado / com falha
â€¢ The batch job is broken because the mapper generated a null pointer.
â€¢ We detected a broken dependency after bumping Spring Boot to 3.x.

Cached â€“ armazenado em cache
â€¢ The product details are cached in Redis to reduce database load.
â€¢ We cached the JWT public keys to avoid constant calls to the authorization server.

Centralized â€“ centralizado
â€¢ All authentication logic is centralized in the identity service.
â€¢ We store feature flags in a centralized configuration provider.

Clean â€“ limpo (cÃ³digo)
â€¢ The new module has a clean architecture, separating domain and infrastructure.
â€¢ We keep controllers thin to maintain clean and readable flows.

Complex â€“ complexo
â€¢ The compensation logic for cancelled orders is complex and event-driven.
â€¢ A complex serialization issue occurs when nested objects are mutated during mapping.

Compliant â€“ em conformidade
â€¢ Our logs must be compliant with PCI-DSS standards.
â€¢ The API is now compliant with the new schema defined in OpenAPI 3.

Concurrent â€“ concorrente
â€¢ We use a concurrent queue to handle high-throughput message processing.
â€¢ Concurrent writes caused race conditions in the legacy module.

Configurable â€“ configurÃ¡vel
â€¢ The retry strategy is configurable through the application YAML.
â€¢ Kafka consumers are fully configurable per environment.

Connected â€“ conectado
â€¢ The service stays connected to MongoDB through a pooled connection.
â€¢ The instance isnâ€™t connected to Redis due to missing network rules.

Consistent â€“ consistente
â€¢ We ensure consistent writes using transactions in Postgres.
â€¢ Consistent logs help us debug distributed flows across microservices.

Critical â€“ crÃ­tico
â€¢ This is a critical path: any delay affects the checkout experience.
â€¢ Kafka offsets are critical for ensuring idempotency.

Current â€“ atual
â€¢ The current version of the service requires Java 21.
â€¢ Our current design uses a decoupled event-driven workflow.

Custom â€“ personalizado
â€¢ We implemented a custom deserializer to handle the partnerâ€™s JSON format.
â€¢ The pipeline uses a custom Jenkins stage to validate OpenAPI contracts.

Decoupled â€“ desacoplado
â€¢ The notification flow is decoupled using Kafka to avoid synchronous waits.
â€¢ We keep business rules decoupled from controllers with the hexagonal approach.

Deprecated â€“ obsoleto
â€¢ The old endpoint is deprecated and will be removed next quarter.
â€¢ Spring marked that annotation as deprecated in Java 21.

Distributed â€“ distribuÃ­do
â€¢ In a distributed system, clock drift can break event ordering.
â€¢ We rely on distributed locks to avoid double-processing orders.

Dynamic â€“ dinÃ¢mico
â€¢ The rule engine loads dynamic conditions from a configuration file.
â€¢ Our dynamic scaling responds to CPU and latency signals.

Effective â€“ eficaz
â€¢ Caching was the most effective solution to reduce database load.
â€¢ The retry policy is effective against transient network failures.

Efficient â€“ eficiente
â€¢ The Kotlin coroutine model is more efficient for IO-bound tasks.
â€¢ We optimized queries to make the billing service more efficient.

Elastic â€“ elÃ¡stico
â€¢ EKS provides an elastic infrastructure to support peak traffic.
â€¢ Elastic queues handle bursts without losing messages.

Encrypted â€“ criptografado
â€¢ All secrets remain encrypted at rest using KMS.
â€¢ The payload is encrypted before being published to Kafka.

Expected â€“ esperado
â€¢ The consumer processed the expected number of messages.
â€¢ The API returned the expected HTTP status code.

External â€“ externo
â€¢ The external payment provider started timing out yesterday.
â€¢ We validate the response schema from the external API.

Failed â€“ falho
â€¢ The batch retried all failed transactions overnight.
â€¢ A failed dependency caused the service to stop during startup.

Flexible â€“ flexÃ­vel
â€¢ The architecture is flexible enough to swap RabbitMQ for Kafka.
â€¢ Our flexible design supports both synchronous and asynchronous flows.

Functional â€“ funcional
â€¢ The new endpoint is functional but needs optimization.
â€¢ We use functional programming patterns with Kotlin flows.

Gradual â€“ gradual
â€¢ We applied a gradual rollout using feature flags.
â€¢ Migration to the new schema will be gradual to avoid downtime.

Heavy â€“ pesado (processamento)
â€¢ The report generator is too heavy to run on the same thread pool.
â€¢ Image processing is heavy, so we moved it to a background worker.

High-level â€“ alto nÃ­vel
â€¢ We present a high-level design before diving into details.
â€¢ Kotlin offers high-level abstractions over concurrency.

Idle â€“ ocioso
â€¢ The service scales down when nodes are idle.
â€¢ Idle connections were causing memory leaks in the legacy pool.

Immutable â€“ imutÃ¡vel
â€¢ Domain events are immutable to guarantee consistency.
â€¢ We keep immutable DTOs to avoid side effects in mapping.

Important â€“ importante
â€¢ Idempotency keys are important for preventing duplicates.
â€¢ Monitoring latency is important in high-throughput systems.

Independent â€“ independente
â€¢ Each microservice is independent and owns its own database.
â€¢ The feature flag service works independent of the main request flow.

Indexed â€“ indexado
â€¢ The customer_id column is indexed to speed up lookups.
â€¢ MongoDB collections were indexed to optimize query patterns.

Informative â€“ informativo
â€¢ We improved logs to be more informative during debugging.
â€¢ The health endpoint returns informative details about each component.

Initial â€“ inicial
â€¢ The initial load of catalog data happens at startup.
â€¢ We ran an initial migration to normalize the order table.

Inline â€“ em linha
â€¢ We removed the inline SQL and moved it to a repository class.
â€¢ Inline mappers caused duplication, so we refactored them into components.

Internal â€“ interno
â€¢ The internal API is not exposed through the gateway.
â€¢ We encrypt all internal service-to-service communication.

Invalid â€“ invÃ¡lido
â€¢ The request returned 400 due to an invalid field in the payload.
â€¢ Kafka listeners skip invalid messages and push them to the DLQ.

Involved â€“ envolvido
â€¢ Multiple services are involved in the checkout flow.
â€¢ Kafka, Redis, and Postgres are involved during the customer login process.

JSON-based â€“ baseado em JSON
â€¢ We standardized our JSON-based events using a shared schema registry.
â€¢ The JSON-based request is validated with Jackson annotations.

Legacy â€“ legado
â€¢ The legacy module still uses JDBC templates and manual transactions.
â€¢ We wrapped the legacy SOAP client behind a modern adapter.

Lightweight â€“ leve
â€¢ Micronaut is more lightweight than Spring for serverless workloads.
â€¢ Our lightweight DTOs speed up serialization under high load.

Limited â€“ limitado
â€¢ The API has limited throughput due to external provider constraints.
â€¢ We use rate limiting because the legacy system has limited capacity.

Local â€“ local
â€¢ We run MongoDB in a local container for integration tests.
â€¢ Local caching reduced latency for hot endpoints.

Logical â€“ lÃ³gico
â€¢ The mapper applies logical transformations to build the domain model.
â€¢ We group business rules in a logical structure inside the domain layer.

Low-level â€“ baixo nÃ­vel
â€¢ We used low-level Kafka APIs for a custom partitioning strategy.
â€¢ Low-level byte manipulation was needed to decode the protocol.

Manual â€“ manual
â€¢ A manual retry was needed because the partner API returned malformed data.
â€¢ We removed manual configuration and moved everything to YAML.

Mandatory â€“ obrigatÃ³rio
â€¢ The customer_id field is mandatory for all POST requests.
â€¢ Authentication is mandatory for accessing the orders endpoint.

Modular â€“ modular
â€¢ Our modular monolith lets us scale components independently.
â€¢ Kotlin modules keep domain logic isolated and clean.

Modern â€“ moderno
â€¢ We are migrating the legacy module to a modern reactive architecture.
â€¢ The service uses modern Java 21 features like virtual threads.

Mutable â€“ mutÃ¡vel
â€¢ Mutable state caused concurrency issues in the billing service.
â€¢ We replaced mutable collections with Kotlinâ€™s immutable List.

Native â€“ nativo
â€¢ We use native AWS SDK integrations to reduce overhead in the payment flow.
â€¢ Quarkus compiled the service to a native binary, improving cold starts.

Networked â€“ ligado Ã  rede
â€¢ All networked components must pass through the companyâ€™s zero-trust gateway.
â€¢ Networked storage introduced extra latency in the reporting module.

Non-blocking â€“ nÃ£o bloqueante
â€¢ WebFlux provides a non-blocking execution model ideal for high concurrency.
â€¢ We migrated the file upload endpoint to a non-blocking approach.

Obvious â€“ Ã³bvio
â€¢ Itâ€™s obvious that the slow query is coming from missing indexes.
â€¢ The NPE was obvious once we checked the mapper logic.

On-demand â€“ sob demanda
â€¢ The service generates reports on-demand using S3 triggers.
â€¢ We scale worker pods on-demand based on queue length.

Optimized â€“ otimizado
â€¢ The new SQL statement is optimized for large datasets.
â€¢ We optimized the serialization process to reduce latency by 20%.

Optional â€“ opcional
â€¢ The filter parameter is optional; we return all records if itâ€™s absent.
â€¢ In Kotlin, Optional values are replaced with nullable types.

Parallel â€“ paralelo
â€¢ The batch process runs in parallel using Java streams.
â€¢ Parallel consumers increased throughput during nightly jobs.

Partial â€“ parcial
â€¢ We applied a partial rollback after a failed transaction.
â€¢ The response includes partial data when the partner API times out.

Persistent â€“ persistente
â€¢ The service writes persistent audit logs to Postgres.
â€¢ Persistent volumes store user-uploaded files across deployments.

Portable â€“ portÃ¡vel
â€¢ Containerized services are portable across environments.
â€¢ Kotlin Multiplatform makes business rules portable to other runtimes.

Predictable â€“ previsÃ­vel
â€¢ Circuit breakers make failures more predictable and controlled.
â€¢ Predictable latency is essential in streaming architectures.

Primary â€“ primÃ¡rio
â€¢ The primary key is a UUID generated on the backend.
â€¢ Our primary goal is to reduce cold starts during peak hours.

Private â€“ privado
â€¢ The private endpoint is only accessible from internal networks.
â€¢ Sensitive fields stay private inside the domain model.

Public â€“ pÃºblico
â€¢ The public API follows strict versioning rules.
â€¢ We published a public contract in the API Gateway.

Random â€“ aleatÃ³rio
â€¢ We generate random salts for password hashing.
â€¢ The load balancer distributes traffic using a random strategy.

Reactive â€“ reativo
â€¢ The login flow is reactive using Mono and Flux.
â€¢ Reactive event streams simplify backpressure handling.

Readable â€“ legÃ­vel
â€¢ We refactored the code to make the validator more readable.
â€¢ Readable logs reduce debugging time in distributed flows.

Real-time â€“ em tempo real
â€¢ Real-time metrics are exported using OpenTelemetry.
â€¢ The fraud system analyzes transactions in real-time.

Redundant â€“ redundante
â€¢ We store redundant events in S3 for replay purposes.
â€¢ Redundant replicas ensure the cluster survives node failures.

Reliable â€“ confiÃ¡vel
â€¢ We moved to Kafka because it provides reliable event delivery under high load.
â€¢ The new retry mechanism made the billing service more reliable.

Remote â€“ remoto
â€¢ The service reads remote configuration from AWS Parameter Store.
â€¢ We replaced the remote SOAP client with a modern REST adapter.

Replicated â€“ replicado
â€¢ Postgres uses replicated nodes to handle read-heavy operations.
â€¢ We replicated the cache to avoid a single point of failure.

Required â€“ obrigatÃ³rio / exigido
â€¢ The token header is required for all protected endpoints.
â€¢ A referenceId is required for every published event.

Responsive â€“ responsivo
â€¢ Our reactive model keeps the API responsive during peak traffic.
â€¢ The dashboard becomes more responsive after caching metrics.

Robust â€“ robusto
â€¢ The hexagonal architecture made the system more robust.
â€¢ A robust error-handling layer prevents cascading failures.

Scalable â€“ escalÃ¡vel
â€¢ The microservice is scalable through horizontal autoscaling.
â€¢ Redis helps the platform remain scalable during Black Friday.

Scheduled â€“ agendado
â€¢ A scheduled job cleans old tokens every six hours.
â€¢ We use a scheduled task to sync data with Salesforce nightly.

Secure â€“ seguro
â€¢ All communication between pods is secure using mTLS.
â€¢ The login flow is secure thanks to OAuth2 and short-lived tokens.

Shared â€“ compartilhado
â€¢ We keep shared DTOs in a contract module used by all microservices.
â€¢ The cluster runs on shared compute resources across teams.

Synchronous â€“ sÃ­ncrono
â€¢ The payment confirmation is a synchronous REST call.
â€¢ We avoid synchronous flows whenever external partners are slow.

Asynchronous â€“ assÃ­ncrono
â€¢ Order notifications are asynchronous via Kafka.
â€¢ We turned the PDF generation into an asynchronous background task.

Temporary â€“ temporÃ¡rio
â€¢ Temporary files are stored in /tmp before being uploaded to S3.
â€¢ We created a temporary fix until the partner releases the new API.

Thread-safe â€“ seguro para threads
â€¢ The singleton mapper is thread-safe because it has no mutable state.
â€¢ ConcurrentHashMap is thread-safe and performs well under load.

Time-consuming â€“ demorado
â€¢ The report generation is time-consuming, so we offloaded it to workers.
â€¢ Schema validation became time-consuming after adding nested relations.

Unavailable â€“ indisponÃ­vel
â€¢ The partner API became unavailable during the maintenance window.
â€¢ Our fallback responds immediately when the service is unavailable.

Unexpected â€“ inesperado
â€¢ We logged an unexpected data format coming from the provider.
â€¢ The consumer crashed due to an unexpected null value.

Valid â€“ vÃ¡lido
â€¢ Only valid events are forwarded to the processing engine.
â€¢ We check if the JWT is still valid before retrieving customer data.

Verbose â€“ verboso (log)
â€¢ The legacy module logs are too verbose and flood Splunk.
â€¢ Verbose stack traces were disabled in production mode.

Virtual â€“ virtual
â€¢ Java 21â€™s virtual threads reduced blocking issues in our API.
â€¢ We run integration tests inside virtual Docker networks.

letÅ› find out more from our individuals
* Letâ€™s find out more from our individuals who handled the Kafka migration; they know the tricky parts.
* Before we redesign the flow, letâ€™s find out more from our individuals that maintain the legacy SOAP service.
* Letâ€™s find out more from our individuals involved in the deployment failure yesterday.

i m a huge fan of scrum
* Iâ€™m a huge fan of Scrum because it keeps the backend team aligned during fast iterations.
* Iâ€™m a huge fan of Scrum; it works really well with microservice development.
* Iâ€™m a huge fan of Scrum, especially when we mix it with technical refinements for architecture

recovery
* The recovery service validates the customerâ€™s identity using JWT and Redis.
* Weâ€™re improving the password recovery flow to reduce latency.
* Recovery is handled through a scheduled job that processes DLQ messages.

fetch
* The BFF fetches customer data from two Java microservices before building the response.
* The consumer fetches offsets from Kafka using a custom strategy.
* We fetch configuration from AWS Parameter Store during startup.

well jump over  to
* Weâ€™ll jump over to the Kafka metrics next, after finishing the API review.
* Weâ€™ll jump over to the Kubernetes manifests once the pipeline is fixed.
* Weâ€™ll jump over to the error-handling rules in a few minutes.

on boarding
* Our onboarding process includes setting up access to Jenkins, Splunk, and the Kubernetes clusters.
* The new dev finished onboarding and already deployed his first Java service.
* Weâ€™re improving onboarding by automating project scaffolding.

mostly i focus on java solutions
* Mostly I focus on Java solutions for async flows and large-scale processing.
* Mostly I focus on Java solutions, but sometimes I contribute to Kotlin features too.
* Mostly I focus on Java solutions with Spring Boot, Kafka, and AWS.

played role of key developer placed role not only in a funcional teams but also in non-funcional requeirement teams
* I played the role of key developer during the migration to Kafka Streams.
* I played the role of key developer when we redesigned the authentication flow.
* I played the role of key developer in the multi-tenant architecture implementation.

â€œfunctional and non-functional requirement teamsâ€
* Iâ€™ve played roles not only in functional teams but also in non-functional requirement teams working on observability.
* I contributed to both functional and non-functional requirement teams for performance tuning.
* I worked with functional and non-functional requirement teams to define SLOs and resiliency patterns.

further
* We need further validation before releasing this feature to production.
* Further analysis showed that the issue came from the thread pool saturation.
* Further improvements will focus on reducing startup time.

letÅ› kick off the interview over to you both
* Letâ€™s kick off the interview â€” over to you both to introduce yourselves.
* Letâ€™s kick off the interview; you both can start with your experience in distributed systems.
* Letâ€™s kick off the interview over to you both, feel free to ask anything about my background.

nonetheless
* The payload was malformed; nonetheless, the service handled it gracefully.
* We had delays in QA; nonetheless, the deployment window remains open.
* The API is stable; nonetheless, we are monitoring for memory leaks.

according
* According to the logs, the error occurred right after the retry attempt.
* According to the metrics, the consumer is falling behind under heavy load.
* According to our SLOs, latency is still within acceptable limits.

responsible for multi-tenance implementarion
* I was responsible for multi-tenancy implementation using schema-per-tenant.
* Iâ€™m currently responsible for multi-tenancy across our Java microservices.
* I was responsible for multi-tenancy validation during CI/CD deployments.

content
* The service fetches content metadata from an external provider.
* We cache content updates in Redis to reduce load on the CMS.
* The BFF aggregates content and product information into a single response.

we work with some third-party content platform
* We work with a third-party content platform that provides all the product descriptions.
* The third-party content platform started returning 500 errors today.
* We integrated the third-party content platform using a resilient retry strategy.

expensive solution
* Running Elasticsearch at full capacity became an expensive solution.
* The partnerâ€™s API is an expensive solution due to high latency and quota limits.
* Storing everything in S3 Glacier was an expensive solution for fast retrieval.

bunch of those
* We have a bunch of those events hitting the DLQ every hour.
* There are a bunch of those legacy endpoints still running on Java 8.
* We fixed a bunch of those NPEs during the refactoring.

warm-up questions
* Before talking about Kafka partitions, letâ€™s start with some warm-up questions.
* These warm-up questions help us understand your experience with Java concurrency.
* I usually ask warm-up questions about REST design before going deeper.

is a job over
* Is the job over after deployment? Not really â€” we still monitor the rollout.
* Is the job over? Only when the metrics show stable performance.
* Is the job over? Not yet, weâ€™re validating the integration with Salesforce.

cleaned up
* We cleaned up the legacy code in the mapper before adding the new business rules.
* The pipeline was failing, but we cleaned up some unused steps and now it works fine.
* I cleaned up the database scripts to remove deprecated columns.

besides
* Besides fixing the bug, I improved the logging to help future debugging.
* Besides Kafka, we also support SQS for asynchronous events.
* Besides Java, I contribute to Kotlin modules when needed.

Behind the scenes, Java resolves itself
* Behind the scenes, Java resolves itself by managing classloading and dependency injection.
* Behind the scenes, Java resolves itself with automatic garbage collection.
* Behind the scenes, Java resolves itself using reflection to handle annotations.

threshold
* The HPA scaled up the pods after the CPU crossed the threshold.
* We defined a latency threshold of 300ms for the payment API.
* When the error threshold is reached, the circuit breaker opens.

heap
* The service crashed because the heap memory was exhausted during batch processing.
* We increased the heap size to handle larger JSON payloads.
* A heap dump revealed a huge map that was never being cleared.

ramdom
* We added a random delay to avoid thundering herd issues.
* The load balancer distributes traffic using a random strategy.
* The ID generator uses a random salt for hashing.

stuff
* Most of the heavy stuff happens inside the order processing service.
* We cleaned out old stuff from the repository to reduce noise.
* The mapper was doing too much stuff, so we split it into smaller components.

huge
* We saw a huge spike in Kafka lag last night.
* Thereâ€™s a huge difference in performance after enabling caching.
* A huge batch of events arrived at once and triggered autoscaling.

he is approaching another
* He is approaching another migration task, this time related to the checkout flow.
* He is approaching another bug related to concurrency and shared state.
* He is approaching another improvement in the retry mechanism.

bring up
* Let me bring up the logs so we can check the root cause.
* We need to bring up the topic of rate limiting during grooming.
* I want to bring up a concern about the cache invalidation strategy.

highlight
* Iâ€™d like to highlight that the consumer is still behind under heavy load.
* Let me highlight a risk in the current design: no idempotency.
* We highlight these endpoints as critical during Black Friday.

shines
* Kotlin really shines when handling coroutines in high-throughput systems.
* Kafka Streams shines in scenarios where real-time aggregation is required.
* The new caching layer shines during peak traffic.

provide
* The adapter must provide a consistent contract to the domain layer.
* Prometheus will provide metrics about memory and CPU usage.
* The BFF will provide consolidated data to the mobile app.

outside
* All outside requests must pass through the API Gateway.
* Weâ€™re receiving outside traffic from a partner system that wasnâ€™t expected.
* Access to Redis from outside the VPC is blocked.

â€œrearlineâ€ (interpreting as â€œbacklineâ€ / â€œbackend supportâ€)
* The rearline team helped us troubleshoot the message loss in Kafka.
* Rearline support identified a misconfigured ACL in the cluster.
* Rearline engineers escalated an issue related to database replication.

achieve it
* We need to reduce latency to 200ms, and caching will help us achieve it.
* Weâ€™re aiming for zero downtime, and blue-green deployment helps us achieve it.
* If we want idempotency, a referenceId is the way to achieve it.

certainly
* This fix will certainly reduce the number of retries in production.
* Switching to Redis will certainly improve response times.
* We will certainly need metrics before scaling this service.

occur
* Most failures occur during the first request after deployment.
* The bug seems to occur only under high concurrency.
* Memory leaks usually occur when mutable objects escape their scope.

nowadays
* Nowadays most Java teams rely on CI/CD fully integrated with Kubernetes.
* Nowadays we monitor everything with OpenTelemetry.
* Nowadays event-driven architectures are becoming the norm.

can swap
* We can swap RabbitMQ for Kafka with minimal impact thanks to our adapter layer.
* The service can swap its cache provider from Redis to DynamoDB if needed.
* We can swap out the ORM without affecting the domain model.

edge cases
* We added extra validation to cover edge cases in the credit simulation API.
* Most bugs happen in edge cases we didnâ€™t consider during mapping.
* The consumer fails only on edge cases where the payload is partially null.

topic
* This topic receives around 20k events per minute during peak hours.
* We created a separate topic for dead-letter messages.
* Each topic follows a naming convention aligned with our domain

itÅ› completely up to up
* You can use WebFlux or MVC â€” itâ€™s completely up to you and the service requirements.
* Whether we deploy today or tomorrow is completely up to me; I own the pipeline.
* We can use either Redis or DynamoDB; itâ€™s completely up to you during design
 
uppercase
* The API rejects the request because the status must be uppercase.
* We convert all headers to lowercase before processing.
* The mapper failed because the field names were mixed between uppercase and lowercase.

even
* Even with caching, the service still hit high latency yesterday.
* Even under load, the consumer stayed stable thanks to backpressure.
* Even a small bug in the mapper can break the entire workflow.

amount
* We validate the amount field before sending the purchase event.
* The amount is calculated using a dedicated domain rule.
* We store the amount in cents to avoid floating-point issues.

configure
* We need to configure the connection pool according to the load profile.
* Iâ€™ll configure the retry policy in the application YAML.
* You can configure the Kafka listener to use manual acknowledgment.

â€œconcernâ€ / â€œpreoccupationâ€
* My main concern is the lack of idempotency in this flow.
* A big concern here is the amount of data we load into memory.
* Thereâ€™s a preoccupation about how the external API handles failures.
 
Look into
* Iâ€™ll look into the logs to understand why the pod crashed.
* We need to look into the mapper; itâ€™s returning null fields.
* Iâ€™ll look into the Grafana dashboard to see the CPU spike.

Work on
* Today Iâ€™ll work on the async retry mechanism.
* We need to work on the integration with the Salesforce API.
* Iâ€™m working on improving the payload validation layer.

it was time to get over / get away my shyness
* had to get over my shyness to start presenting architecture proposals.
* At standups, I got away from my shyness and started communicating better.
* Preparing interviews helped me get over my shyness when speaking English.

Thatâ€™s why I got in / got back
* The team needed help with Kafka, thatâ€™s why I got in to support the migration.
* The incident escalated, thatâ€™s why I got back online during the night.
* They needed Java expertise, thatâ€™s why I got in on the project.

* Every week I carried out / looked up new stories and afterward I had a great time telling them
* Every week I carried out new stories about Kafka consumers and dead letters.
* Every week I looked up new stories related to performance tuning.
* Every week I carried out integration stories with external APIs.

as well
* We need to update the YAML files as well.
* The mapper needs adjustments, and the service layer as well.
* I fixed the Kafka config as well during the deployment.

We get up / get along really well, and I donâ€™t feel shy at all around them
* The backend team gets along really well, especially during refactoring sessions.
* We get along really well with the SRE team; deployments run smoothly.
* The Java and mobile teams get along really well for API design.

silver bullet
* Caching is good, but itâ€™s not a silver bullet for performance problems.
* Kotlin coroutines help, but theyâ€™re not a silver bullet for concurrency.
* Kafka is powerful, but itâ€™s not a silver bullet for all async flows.

increase
* We need to increase the consumer thread count to avoid lag.
* Autoscaling will increase the replicas during high traffic.
* Caching increased the throughput by 30%.

stuff
* The controller still does too much stuff; we need to refactor it.
* Letâ€™s remove unused stuff from the YAML configurations.
* The CI pipeline had a bunch of old stuff that slowed it down.

grooming sessions
* During grooming sessions, we define acceptance criteria more clearly.
* We identified several tech debts in the last grooming session.
* Grooming sessions help us estimate backend tasks more accurately.

Then he covered the topic of Spring knowledge
* Then he covered the topic of Spring knowledge, especially dependency injection.
* Then he covered the topic of Spring knowledge during the architecture interview.
* Then he covered the topic of Spring knowledge, focusing on WebFlux vs MVC.

With that in mind
* With that in mind, we should apply caching before scaling the pods.
* With that in mind, letâ€™s redesign the API to be fully idempotent.
* With that in mind, the migration to Kafka Streams becomes the best option.

throwble shotting
* We spent the afternoon troubleshooting a memory leak in the mapper.
* Troubleshooting the consumer lag showed a misconfigured partition key.
* Most of our troubleshooting came from missing metrics in Prometheus.

its very cute
* The new UI for the Swagger docs is very cute, but the backend still needs fixes.
* This Kotlin DSL is very cute â€” simple and expressive.
* The feature flag dashboard looks very cute after the redesign.

I could start by talking about....
* I could start by talking about my experience with Java 21 and virtual threads.
* I could start by talking about the multi-tenant architecture I implemented.
* I could start by talking about how we redesigned the Kafka consumer logic.

assesment
* We did a performance assessment before rewriting the service.
* Our reliability assessment showed missing retries in two APIs.
* A security assessment revealed outdated libraries.

reduce coupling
* To reduce coupling, we isolated all business logic inside domain services.
* Using events helps reduce coupling between checkout and payment services.
* Adapters allow us to reduce coupling with external providers.

let's keep rooling
* The tests are passing; letâ€™s keep rolling with the integration phase.
* We fixed the retry logic â€” letâ€™s keep rolling and deploy to QA.
* Standup is quick today; letâ€™s keep rolling with the backlog items.

despite that
* The API is slow; despite that, the system is still stable under load.
* We hit a few errors yesterday; despite that, the feature is ready for QA.
* Deployment took longer; despite that, the health checks passed.

we might be talking about a few things
* If we talk about scaling, we might be talking about a few things: load, memory, and throughput.
* When dealing with caching, we might be talking about a few things like eviction strategy and TTL.
* For observability, we might be talking about a few things such as logs, metrics, and traces.

tightly
* The controller and service were tightly coupled, so we refactored them.
* The legacy module is tightly integrated with a SOAP provider.
* The threads were tightly competing for the same lock.

stackholders
* We aligned the SLO changes with all stakeholders last week.
* Stakeholders requested a rollback after the spike in latency.
* I presented the architecture proposal to the stakeholders.

concern
* A major concern here is the lack of retries for external APIs.
* My concern is how the cache invalidation will behave under load.
* Another concern is how we guarantee ordering between events.

let me put this way
* Let me put it this way: without idempotency, the checkout flow is unsafe.
* Let me put it this way: scaling solves symptoms, not the root cause.
* Let me put it this way: if the mapper breaks, the whole domain breaks.

â€œensuranceâ€ (interpreting as â€œassuranceâ€ / â€œguaranteeâ€)
* We added extra logging as an assurance for debugging incidents.
* Code reviews give us assurance that critical flows stay consistent.
* The retry layer provides assurance during peak traffic.

aproaches
* There are two approaches: synchronous API or event-driven flow.
* We evaluated different approaches before choosing Redis.
* Reactive and imperative approaches both work depending on the load.

He is thorough when reviewing pull requests.
* He is thorough when reviewing pull requests, especially around threading issues.
* He is thorough when reviewing pull requests related to domain rules.
* He is thorough when reviewing pull requests involving database migrations.

sometimes I get annoyed
* Sometimes I get annoyed when the external API fails for no reason.
* Sometimes I get annoyed with noisy logs flooding the console.
* Sometimes I get annoyed when the pipeline breaks because of a plugin.

otherwise
* We need a unique ID; otherwise, events will not be idempotent.
* Use backpressure; otherwise, the consumer will lag behind.
* Add validation; otherwise, the request will break downstream services.

I am eager to take on new challenges and grow as a backend developer.       
* I am eager to take on new challenges like designing high-throughput systems.
* I am eager to take on new challenges involving Kotlin coroutines.
* I am eager to take on new challenges in distributed architecture.

tight coupling
* The service failed because of tight coupling between controller and repository.
* We removed tight coupling by creating an adapter layer.
* Tight coupling makes it harder to migrate to Kafka.

besides
* Besides fixing the unit tests, I also improved the mapper performance.
* Besides the latency issue, we also found memory pressure in the heap.
* Besides Kafka, we have to support SQS for legacy integration.

pivotal
* Caching played a pivotal role in reducing database load.
* The retry layer is pivotal for keeping resilience under unstable networks.
* Monitoring is pivotal to detecting slowdowns before they escalate

purpose
* The purpose of this service is to centralize authentication.
* The purpose of the adapter is to isolate external APIs from the domain.
* The purpose of this migration is to improve throughput.

certanlyk  
* This refactoring will certainly make the code easier to maintain.
* Switching to WebFlux will certainly increase capacity for IO-bound requests.
* Using Redis will certainly lower latency for hot endpoints

he reinforced
* He reinforced the importance of adding idempotency keys.
* He reinforced the need to check metrics before scaling.
* He reinforced that the mapper must stay pure and immutable.

â€œto policeâ€ (no sentido de â€œvigiarâ€, â€œcontrolarâ€, â€œsupervisionarâ€)
* We added alerts to police CPU spikes in production.
* Feature flags help police risky code paths.
* We police API quotas to avoid overload from external partners.

guarantee
* We use transactions to guarantee consistency.
* The partition key guarantees event ordering.
* Retries guarantee delivery even when the partner API is unstable.

in fact
* In fact, the mapper wasnâ€™t the problem â€” the payload was wrong.
* In fact, Redis reduced latency more than scaling the pods.
* In fact, the consumer was healthy; the issue was in the producer.

international position
* Iâ€™m preparing my English because Iâ€™m aiming for an international position.
* An international position requires strong communication across time zones.
* I handled architecture discussions as part of an international position in a previous project.

â€œacknowledgeâ€ (corrigindo â€œacknologeâ€)
* The API must acknowledge the event before persisting it.
* Kafka consumers manually acknowledge messages in this flow.
* The service didnâ€™t acknowledge the 200 response because of a timeout.

throughput
* Increasing partitions usually raises the throughput of the consumer.
* The new design achieved 5Ã— more throughput.
* Throughput dropped because of backpressure in the downstream API.

back pressure
* WebFlux applies back pressure automatically to avoid overload.
* Back pressure prevents the consumer from crashing under high traffic.
* Without back pressure, Kafka lag would grow uncontrollably.

according to your needs
* We can tune the retry policy according to your needs.
* The API scales horizontally according to your needs.
* We can choose between Postgres or DynamoDB according to your needs.

Depending on his needs, he requests a piece of information.
* Depending on his needs, he requests a piece of information from the user profile service.
* Depending on his needs, he requests a piece of information from the cache or the database.
* Depending on his needs, he requests a piece of information from the audit logs.

expensive and cheap
* Database reads are cheap, but cross-region writes are expensive.
* Serializing small DTOs is cheap; transforming large JSON trees is expensive.
* Horizontal scaling is cheap; vertical scaling is expensive.

benchmarks
* We ran benchmarks to compare WebFlux with MVC under heavy load.
* Benchmarks showed that Redis outperforms the in-memory cache for large payloads.
* Our benchmarks revealed that the mapper was the real bottleneck.

upcoming
* Weâ€™re preparing for the upcoming release and stabilizing all services.
* Upcoming changes will affect how we handle authentication tokens.*
* Upcoming load tests will validate the new scaling strategy.

even under load
* The service stayed stable even under load during Black Friday.
* Redis kept latency low even under load.
* Kafka processed events consistently even under load.

---
> 100 Most Useful English Verbs for IT Professionals
1â€“20: Core Daily Workflow

Build â€“ compile or assemble
We build the project using Maven.
Deploy â€“ send to an environment
We deploy to QA every morning.
Release â€“ publish a new version
We will release 1.2.0 today.
Run â€“ execute
Run the tests before committing.
Test â€“ verify something
Test the endpoint locally.
Fix â€“ correct a problem
I fixed the null pointer issue.
Debug â€“ analyze errors
Letâ€™s debug the failing service.
Improve â€“ make better
We need to improve performance.
Review â€“ check code or docs
Can you review my PR?
Merge â€“ combine branches
We merge to main only after approval.
Commit â€“ send code to Git
Commit with a clear message.
Push â€“ upload to remote
Push your branch when ready.
Pull â€“ download changes
Pull the latest updates before developing.
Refactor â€“ reorganize code
We should refactor this service.
Document â€“ write documentation
Please document the API behavior.
Configure â€“ set parameters
Configure the Kafka consumer correctly.

Optimize â€“ make more efficient
We optimized the SQL queries.

Validate â€“ check correctness
Validate the input before saving.

---
â€œSlightlyâ€ Ã© muito usado em TI â€” e de um jeito natural e frequente.
Ele significa:
âœ… um pouco / levemente / de forma sutil
Ã‰ perfeito para falar de desempenho, latÃªncia, diferenÃ§as entre abordagens, ajustes em cÃ³digo, variaÃ§Ãµes de carga etc.
ğŸ“Œ Exemplos em contexto de backend/Java/Kubernetes
â€œThis version is slightly faster than the previous one.â€
(Essa versÃ£o estÃ¡ um pouco mais rÃ¡pida.)
â€œThe latency increased slightly after the last deployment.â€
(A latÃªncia aumentou levemente.)
â€œWe need to slightly adjust the memory limits in the Kubernetes manifest.â€
(Precisamos ajustar um pouco os limites de memÃ³ria no manifest.)
â€œThe new algorithm uses slightly less CPU under heavy load.â€
(O novo algoritmo usa um pouco menos de CPU sob carga pesada.)
â€œThe logs were slightly misleading during the incident.â€
(Os logs estavam ligeiramente confusos durante o incidente.)
â€œThe architecture is slightly different from what we expected.â€
(A arquitetura Ã© um pouco diferente do que esperÃ¡vamos.)

ğŸ¯ Resumo
Slightly = â€œum poucoâ€ de forma suave â†’ super usado em relatÃ³rios de performance, incidentes, anÃ¡lises tÃ©cnicas e code reviews.
Se quiser, faÃ§o mais frases usando slightly em cenÃ¡rios de CI/CD, Kafka, Redis, JPA ou microservices.

---